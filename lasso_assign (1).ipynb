{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b0c2911-ec6f-4325-bb7c-50d1a967ed8d",
   "metadata": {},
   "source": [
    "Q1. What is Lasso Regression, and how does it differ from other regression techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c04b10-bb47-40e4-b19b-323e3a55ac9e",
   "metadata": {},
   "source": [
    "It is also known as L1 regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ec21dc-9e43-4d3f-97b0-abe3e7f3a826",
   "metadata": {},
   "source": [
    "It is used in feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a5321c-2cfe-4c9f-a355-38ea4520e77a",
   "metadata": {},
   "source": [
    "Lasso Regression, short for Least Absolute Shrinkage and Selection Operator, is a type of linear regression technique used for feature selection and regularization. It's particularly effective when dealing with datasets that have a large number of features, some of which may not be relevant or may have multicollinearity issues (where predictors are highly correlated)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ed5860-5472-470b-adb3-f95682ea01f2",
   "metadata": {},
   "source": [
    "In Ridge regression , it used to reduce overfitting where Lasso regression It is used when features are in large number."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a86078",
   "metadata": {},
   "source": [
    "Q2. What is the main advantage of using Lasso Regression in feature selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4d1a97-5f4d-425a-90d2-71c5360cf9fd",
   "metadata": {},
   "source": [
    "Reduction of Overfitting: By reducing the number of features and hence the complexity of the model, Lasso Regression helps prevent overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72a8233-c3fd-49e0-a576-7623641804f8",
   "metadata": {},
   "source": [
    "Dealing with Multicollinearity: Lasso Regression is effective in handling multicollinearity issues, where predictors are highly correlated. It tends to select one variable from a group of highly correlated variables and set the coefficients of others to zero, thus dealing with redundant information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31141e8-7f54-432b-b0c8-4901fa746ff5",
   "metadata": {},
   "source": [
    "Sparse Models: Lasso Regression tends to produce sparse models by setting coefficients of less important or irrelevant features to zero. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf53a0cf-d595-416c-a923-539e99541cd2",
   "metadata": {},
   "source": [
    "Q3. How do you interpret the coefficients of a Lasso Regression model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1b7174-ba02-4a77-a832-e16948c8c0a7",
   "metadata": {},
   "source": [
    "Non-zero Coefficients: The non-zero coefficients indicate the variables that the model has deemed important for predicting the target variable. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a6129b-63a8-4444-80ed-e8cfa605a78d",
   "metadata": {},
   "source": [
    "Zero Coefficients: Coefficients that have been shrunk to zero by the Lasso Regression indicate variables that the model has effectively excluded. These variables are considered unimportant or less relevant for predicting the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9cf52f-7978-47ab-8da8-7f1d22aad6c7",
   "metadata": {},
   "source": [
    "Magnitude of Coefficients: The magnitude of the non-zero coefficients in Lasso Regression is still indicative of the importance of each feature. Larger absolute values suggest a stronger impact on the target variable. Comparing the magnitudes of coefficients can give a relative idea of which predictors have a more significant influence on the predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bd3848-1fd2-4beb-b681-ac8c3d97e48e",
   "metadata": {},
   "source": [
    "Q4. What are the tuning parameters that can be adjusted in Lasso Regression, and how do they affect the\n",
    "model's performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595141e1-1080-4987-a51d-26c61788a156",
   "metadata": {},
   "source": [
    "Lambda : Larger alpha values result in stronger regularization, leading to more coefficients being shrunk to zero. This increases the sparsity of the model and helps in feature selection.\n",
    "Smaller alpha values decrease the penalty on the coefficients, allowing more coefficients to remain non-zero and resulting in a less sparse model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a52be9-eab3-434a-a5fa-f4b5631255b0",
   "metadata": {},
   "source": [
    "Max Iterations: This parameter specifies the maximum number of iterations for the optimization algorithm to converge. As Lasso Regression involves solving an optimization problem to find the optimal coefficients, setting an appropriate number of iterations ensures convergence to the solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105cda1b-7817-4692-a5d4-2ec1804dcbb2",
   "metadata": {},
   "source": [
    "Regularization Strength: Increasing alpha strengthens the penalty on coefficients, encouraging more coefficients to be set to zero, thus performing more aggressive feature selection. This helps prevent overfitting and improves the model's generalization ability, especially when dealing with high-dimensional datasets with many irrelevant features.\n",
    "\n",
    "Model Complexity: Larger alpha values lead to simpler and more interpretable models with fewer features retained, while smaller alpha values allow for more complex models with a higher number of non-zero coefficients. Therefore, the choice of alpha affects the trade-off between model simplicity and predictive power.\n",
    "\n",
    "Computational Efficiency: The choice of alpha can also affect the computational complexity of the model. Stronger regularization (higher alpha values) might converge faster due to sparser solutions, requiring fewer iterations, while smaller alpha values might demand more computational resources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae922a9-60c4-4931-bf31-bb6be85153b6",
   "metadata": {},
   "source": [
    "Q5. Can Lasso Regression be used for non-linear regression problems? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721a4140-79c8-4cb2-8055-0e7dcf868dbb",
   "metadata": {},
   "source": [
    "On the one hand, Lasso is by design a method for linear regression, and it is dubious to expect it to work if y depends non-linearly on Ax. On the other hand, practitioners have been successfully using Lasso for non-linear (especially binary) observations without theoretical backing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7838ddb1-9f01-41ca-9566-e00e9736d501",
   "metadata": {},
   "source": [
    "Q6. What is the difference between Ridge Regression and Lasso Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e41679-74c8-4e37-b0e9-e03fe68c175a",
   "metadata": {},
   "source": [
    "Ridge Regression and Lasso Regression are both regularization techniques used in linear regression to address multicollinearity and overfitting issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5feb72-b506-4667-9e7b-84d43ff3f3d1",
   "metadata": {},
   "source": [
    "Ridge uses the L2 norm penalty, which results in all coefficients being shrunk towards zero without necessarily being set exactly to zero. Lasso uses the L1 norm penalty, which can drive some coefficients exactly to zero, leading to feature selection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df2c650-872d-491f-a138-30aa8fdbf252",
   "metadata": {},
   "source": [
    "Similar to the lasso regression, ridge regression puts a similar constraint on the coefficients by introducing a penalty factor. However, while lasso regression takes the magnitude of the coefficients, ridge regression takes the square. Ridge regression is also referred to as L2 Regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9bbf04-8dc0-4853-bd54-da7628a68306",
   "metadata": {},
   "source": [
    "Q7. Can Lasso Regression handle multicollinearity in the input features? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0dcef6b-9ac5-453b-afbd-e8821078c262",
   "metadata": {},
   "source": [
    "Yes, Lasso Regression has the ability to handle multicollinearity, which occurs when input features are highly correlated with each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d5b65f-e040-428c-8ee4-47d59c2e09af",
   "metadata": {},
   "source": [
    "Variable Selection: Lasso Regression tends to perform automatic variable selection by shrinking coefficients and forcing some of them to be exactly zero. When faced with highly correlated features, Lasso Regression tends to select one feature from a group of correlated features and sets the coefficients of others to zero.\n",
    "\n",
    "Sparsity Inducing Property: The L1 regularization penalty used in Lasso Regression encourages sparsity in the model. This means that when there are highly correlated features, Lasso Regression might select one feature while reducing the coefficients of the remaining correlated features to zero. This process effectively deals with multicollinearity by choosing one representative feature and disregarding the redundant ones.\n",
    "\n",
    "Feature Subset Selection: By zeroing out coefficients of less important or redundant features, Lasso Regression naturally performs feature subset selection, favoring only the most relevant predictors while ignoring others. In the presence of multicollinearity, it tends to favor one feature over the others in a correlated group, effectively handling the issue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1215f41-5b3a-4246-8498-db23236ca403",
   "metadata": {},
   "source": [
    "Q8. How do you choose the optimal value of the regularization parameter (lambda) in Lasso Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3603c296-f9f7-4b04-8f83-03c0d1882761",
   "metadata": {},
   "source": [
    "When choosing a lambda value, the goal is to strike the right balance between simplicity and training-data fit: If your lambda value is too high, your model will be simple, but you run the risk of underfitting your data. Your model won't learn enough about the training data to make useful predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07549c8-0ab6-472b-b5bb-64e74c8d9827",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
